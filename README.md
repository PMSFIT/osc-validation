# osc-validation

This repository provides a validation suite for ASAM OpenSCENARIO XML engines based on comparing their OSI ground_truth/sensor_view output to known good references.

The validation is based on a [subset definition](specification/osc_subset_definition.md) that restricts the covered constructs of OpenSCENARIO XML to a minimal, unambiguous, validatable, but useful subset.

It derives from this subset a validation suite of representative test cases and metrics that can then be used to check an implementation for correct OSI ground truth generation for those cases.

## Installation

Use [poetry](https://python-poetry.org/) to install the validation suite into a new virtual environment:

```bash
poetry install
```

## Execution

Run the following command to show all pytest options as well as the OSC Validation Suite options.

```bash
pytest validation/scenario -h
```

The first argument (`validation/scenario`) specifies the test directory to run. Pytest will recursively discover and execute all test files within this folder based on its `pytest.ini` configuration, using it as the root for test collection.

Run the following command to execute the test suite using the specified test directory with the chosen tool to be validated.

```bash
pytest validation/scenario --tool <TOOL_NAME> --toolpath <PATH_TO_TOOL_EXECUTABLE>
```

Replace `<TOOL_NAME>` with the name of the OpenSCENARIO engine you want to test (e.g., `ESMini`) and `<PATH_TO_TOOL_EXECUTABLE>` with the path to its executable file.

### Example

```bash
pytest validation/scenario --tool ESMini --toolpath C:/path/to/esmini/bin/esmini.exe
```

Additional pytest options can be used as needed, such as `-v` for verbose output or `-k` to run specific tests by specifying their function name.

For more information on available command-line options, run:

```bash
pytest --help
```

## Validation setup

There are certain required components to setup a tool validation process with OSC Validation Suite:

- **Tool integration:** Create a tool wrapper for automated trace generation with a desired OpenSCENARIO XML engine.
- **Validation metrics:** Create or integrate validation metrics (based on a certain benchmark format, e.g. OSI SensorView traces).
- **Reference implementation:** Create or integrate a reference implementation adequate for the desired test cases (e.g. OSI to OpenSCENARIO XML converter).
- **Validation resources:** Add validation reference resources (OSI traces, OpenDRIVE map, etc.) by using the data provider functionality and pytest fixtures.
- **Test case design:** Build test cases by combining the reference resources, a reference implementation, automated trace generation, validation metrics and corresponding test parameters.

> **Note:** The validation suite is primarily designed for validating OpenSCENARIO XML engines that output OSI trace files, but it can also be applied generically to other tools and file formats.

### Tool integration

To automate and harmonize the invocation of different tools in the validation process, tool wrapper classes can be created.
A tool wrapper class is responsible for interacting with the tool, e.g. running CLI commands or interacting with a GUI (e.g. by using playwright automation library).
OSC Validation Suite already supports **ESMini** and **GTGen Simulator** (gtgen_cli) as exemplary OpenSCENARIO XML engines.
The wrapper class can also contain code to post-process a tool's output data to ensure compatibility with other validation resources (e.g. convert ESMini OSI GroundTruth traces into standard-compliant OSI SensorView traces).
Note that the tool-specific post-processing should only involve tool-specific modifications and should not include generic post-processing functionality.

#### How to integrate a tool

Create a subclass based on the the base class interface defined in [osc_validation/tools/osctool.py](./osc_validation/tools/osctool.py) to create a custom tool wrapper class.
```python
class OSCTool:
    def __init__(self, tool_path=None):
        self.tool_path = Path(tool_path) if tool_path else None

    def run(self, osc_path, odr_path, osi_path):
        raise NotImplementedError("This method must be implemented by the subclass.")
```

The method `run` is responsible for calling the respective tool (e.g. via `os.system` call) with the desired arguments.
It must return the path to the output trace generated by the tool.

The file [validation/scenario/conftest.py](./validation/scenario/conftest.py) adds custom pytest cli options and hooks specific to the OSC Validation Suite.
A new tool wrapper class must be integrated into the test suite by specifying a tool name option for the pytest cli in `conftest.py` and adding a new conditional clause for it as follows:
```python
def pytest_addoption(parser):
   # [...]
   group.addoption(
      "--tool", action="store", default="ESMini", help="Tool to Validate: ESMini, GTGen, YourToolName"
   )
   # [...]

@pytest.fixture(scope="session")
def generate_tool_trace(request):
   # [...]
    if tool_name == "ESMini":
        tool = ESMini(request.config.getoption("--toolpath"))
    elif tool_name == "GTGen":
        tool = GTGen_Simulator(request.config.getoption("--toolpath"))
    elif tool_name == "YourToolName":
        tool = YourToolWrapperClass(request.config.getoption("--toolpath"))
    else:
        tool = None
   # [...]
```

### Validation metrics

Validation metrics can be created independently of test cases.
Validation metric functions should receive both the validation reference file path and the tool-generated file path.
There can be certain preconditions that both files have to meet (e.g. same frame rate, same number of frames, matching object ids) but these preconditions usually depend on the implemented metric.
Certain preconditions can be considered by initializing the tool with suitable arguments (e.g. set step size, set stop).
Other preconditions must be taken care of by post-processing the tool output trace in the test case definition to ensure proper comparability across the two trace files (e.g. cropping trace length).

### Reference implementation

A reference implementation serves as a baseline for correctness and conformity and defines the expected outcome of the tool under test.
It is not expected to provide full feature coverage beyond the test scope but minimally provide the functionality covered by the selected test cases.
The provided [OSI to OpenSCENARIO XML Converter](./osc_validation/generation/osi2osc.py) covers the functionality of the [defined OpenSCENARIO subset](./specification/osc_subset_definition.md).
It can be extended to support further features or supplemented with further separate reference implementations.
Selection and invocation of the desired reference implementation is part of the test case design.

### Validation resources

OSC Validation Suite provides a flexible data provider system that can supply test or validation data from local built-in directories or by downloading and extracting remote ZIP archives on demand.
Such resources are prepared for provision to a test case via pytest fixtures:

```python
@pytest.fixture(
    scope="module",
    params=["simple_trajectories/sv_trace1.osi", "simple_trajectories/sv_trace2.osi"],
)
def osi_trace(request):
    provider = BuiltinDataProvider()
    yield provider.ensure_data_path(request.param)
    provider.cleanup()
```

### Test case design

Test cases are built using the previously described components and pytest's built-in functionality.
Pytest fixtures allow to define and use common test dependencies like validation resources (`osi_trace`, `odr_file`) or tool execution processes (`generate_tool_trace`).
Note that pytest fixtures can be declared with different scopes (function, class, module, package, or session) which define when the fixture is created and when it's torn down during test execution.
Fixtures are injected into test case functions via parameters.
Default pytest fixtures (e.g. `tmp_path`) can also be used as expected. Run `pytest --fixtures` to display all options.
```python
def example_test_case(osi_trace, odr_file, generate_tool_trace, tmp_path, moving_object_id, tolerance=1e-6):
```
A reference implementation (e.g., the OSI to OpenSCENARIO XML Converter osi2osc) is used to generate an OpenSCENARIO file from a baseline OSI reference trace, which serves as input for the tool under test.
The callable fixture `generate_tool_trace` is then used to automatically produce the corresponding tool-generated trace.
This output can be compared against the reference trace using predefined validation metrics and tolerances.
If necessary, post-processing steps, such as cropping, can be applied to the tool-generated trace to ensure alignment with the reference.

Built-in or custom pytest markers facilitate test execution control (e.g. skip, select, categorize, parametrize test cases).
For example, the `@pytest.mark.parametrize` decorator is used to run a test function multiple times with different sets of arguments:
```python
@pytest.mark.parametrize("moving_object_id", [1, 2, 3])
```

Custom pytest markers can be added in `pytest.ini` or programatically in `conftest.py`.
